{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this, we are predicting the admission on the basis of diff. exam given in the dataset.  \n",
    "From kaggle => (https://www.kaggle.com/datasets/mohansacharya/graduate-admissions?select=Admission_Predict_Ver1.1.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500</td>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0             1        337          118                  4  4.5   4.5  9.65   \n",
       "1             2        324          107                  4  4.0   4.5  8.87   \n",
       "2             3        316          104                  3  3.0   3.5  8.00   \n",
       "3             4        322          110                  3  3.5   2.5  8.67   \n",
       "4             5        314          103                  2  2.0   3.0  8.21   \n",
       "..          ...        ...          ...                ...  ...   ...   ...   \n",
       "495         496        332          108                  5  4.5   4.0  9.02   \n",
       "496         497        337          117                  5  5.0   5.0  9.87   \n",
       "497         498        330          120                  5  4.5   5.0  9.56   \n",
       "498         499        312          103                  4  4.0   5.0  8.43   \n",
       "499         500        327          113                  4  4.5   4.5  9.04   \n",
       "\n",
       "     Research  Chance of Admit   \n",
       "0           1              0.92  \n",
       "1           1              0.76  \n",
       "2           1              0.72  \n",
       "3           1              0.80  \n",
       "4           0              0.65  \n",
       "..        ...               ...  \n",
       "495         1              0.87  \n",
       "496         1              0.96  \n",
       "497         1              0.93  \n",
       "498         0              0.73  \n",
       "499         0              0.84  \n",
       "\n",
       "[500 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/Hp/Downloads/Deep Learning/Admission_Predict_Ver1.1.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRE Scores=> out of 340.  \n",
    "TOEFL Scores=> out of 120.  \n",
    "University Rating=> out of 5.  \n",
    "DOP=> Statement of Purpose ( out of 5 ).  \n",
    "LOR=> Letter of Recommendation Strength ( out of 5 ).  \n",
    "CGPA=> out of 10.  \n",
    "Research=> \"0\" or \"1\".  \n",
    "Chance of Admit=> from \"0\" to \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         500 non-null    int64  \n",
      " 1   GRE Score          500 non-null    int64  \n",
      " 2   TOEFL Score        500 non-null    int64  \n",
      " 3   University Rating  500 non-null    int64  \n",
      " 4   SOP                500 non-null    float64\n",
      " 5   LOR                500 non-null    float64\n",
      " 6   CGPA               500 non-null    float64\n",
      " 7   Research           500 non-null    int64  \n",
      " 8   Chance of Admit    500 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 35.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the \"null values\" and \"Duplicate values\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial No.           0\n",
       "GRE Score            0\n",
       "TOEFL Score          0\n",
       "University Rating    0\n",
       "SOP                  0\n",
       "LOR                  0\n",
       "CGPA                 0\n",
       "Research             0\n",
       "Chance of Admit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the first column.  \n",
    "So will drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0          337          118                  4  4.5   4.5  9.65         1   \n",
       "1          324          107                  4  4.0   4.5  8.87         1   \n",
       "2          316          104                  3  3.0   3.5  8.00         1   \n",
       "3          322          110                  3  3.5   2.5  8.67         1   \n",
       "4          314          103                  2  2.0   3.0  8.21         0   \n",
       "..         ...          ...                ...  ...   ...   ...       ...   \n",
       "495        332          108                  5  4.5   4.0  9.02         1   \n",
       "496        337          117                  5  5.0   5.0  9.87         1   \n",
       "497        330          120                  5  4.5   5.0  9.56         1   \n",
       "498        312          103                  4  4.0   5.0  8.43         0   \n",
       "499        327          113                  4  4.5   4.5  9.04         0   \n",
       "\n",
       "     Chance of Admit   \n",
       "0                0.92  \n",
       "1                0.76  \n",
       "2                0.72  \n",
       "3                0.80  \n",
       "4                0.65  \n",
       "..                ...  \n",
       "495              0.87  \n",
       "496              0.96  \n",
       "497              0.93  \n",
       "498              0.73  \n",
       "499              0.84  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=[\"Serial No.\"], axis=1, inplace= True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will spilt the dataset into \"Dependent\" and \"Independent\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "0          337          118                  4  4.5   4.5  9.65         1\n",
       "1          324          107                  4  4.0   4.5  8.87         1\n",
       "2          316          104                  3  3.0   3.5  8.00         1\n",
       "3          322          110                  3  3.5   2.5  8.67         1\n",
       "4          314          103                  2  2.0   3.0  8.21         0\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "495        332          108                  5  4.5   4.0  9.02         1\n",
       "496        337          117                  5  5.0   5.0  9.87         1\n",
       "497        330          120                  5  4.5   5.0  9.56         1\n",
       "498        312          103                  4  4.0   5.0  8.43         0\n",
       "499        327          113                  4  4.5   4.5  9.04         0\n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.iloc[:,0:-1]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.92\n",
       "1      0.76\n",
       "2      0.72\n",
       "3      0.80\n",
       "4      0.65\n",
       "       ... \n",
       "495    0.87\n",
       "496    0.96\n",
       "497    0.93\n",
       "498    0.73\n",
       "499    0.84\n",
       "Name: Chance of Admit , Length: 500, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.iloc[:,-1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will Standardize the \"x\" and \"y\" by \"MinMaxScalar\", beacuse we know the max limit/value of each variables in both \"x\" and \"y\" variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max = MinMaxScaler()\n",
    "\n",
    "x_train_scale = min_max.fit_transform(x_train)\n",
    "x_test_scale = min_max.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4       , 0.51851852, 0.5       , ..., 0.625     , 0.59935897,\n",
       "        0.        ],\n",
       "       [0.56      , 0.2962963 , 1.        , ..., 1.        , 0.63461538,\n",
       "        1.        ],\n",
       "       [0.58      , 0.62962963, 0.5       , ..., 0.375     , 0.63782051,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.7       , 0.51851852, 0.5       , ..., 0.625     , 0.74038462,\n",
       "        1.        ],\n",
       "       [0.36      , 0.33333333, 0.25      , ..., 0.625     , 0.37820513,\n",
       "        1.        ],\n",
       "       [0.44      , 0.59259259, 0.25      , ..., 0.75      , 0.71153846,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.32      ,  0.44444444,  0.25      ,  0.5       ,  0.375     ,\n",
       "         0.46794872,  0.        ],\n",
       "       [ 0.42      ,  0.33333333,  0.5       ,  0.875     ,  0.75      ,\n",
       "         0.58974359,  1.        ],\n",
       "       [ 0.12      ,  0.14814815,  0.25      ,  0.125     ,  0.25      ,\n",
       "         0.32051282,  0.        ],\n",
       "       [ 0.42      ,  0.22222222,  0.        ,  0.375     ,  0.5       ,\n",
       "         0.5224359 ,  1.        ],\n",
       "       [ 0.7       ,  0.66666667,  0.75      ,  0.75      ,  0.875     ,\n",
       "         0.74038462,  1.        ],\n",
       "       [ 0.64      ,  0.62962963,  0.5       ,  0.625     ,  0.5       ,\n",
       "         0.69230769,  1.        ],\n",
       "       [ 0.48      ,  0.51851852,  0.25      ,  0.375     ,  0.75      ,\n",
       "         0.56410256,  0.        ],\n",
       "       [ 0.56      ,  0.62962963,  0.        ,  0.375     ,  0.625     ,\n",
       "         0.55769231,  1.        ],\n",
       "       [ 0.18      ,  0.25925926,  0.        ,  0.125     ,  0.25      ,\n",
       "         0.34935897,  0.        ],\n",
       "       [ 0.2       ,  0.07407407,  0.25      ,  0.5       ,  0.125     ,\n",
       "         0.45512821,  1.        ],\n",
       "       [ 0.96      ,  0.81481481,  1.        ,  0.875     ,  1.        ,\n",
       "         0.77884615,  1.        ],\n",
       "       [ 0.28      ,  0.51851852,  0.5       ,  0.625     ,  0.5       ,\n",
       "         0.33974359,  0.        ],\n",
       "       [ 0.78      ,  0.62962963,  0.25      ,  0.75      ,  0.5       ,\n",
       "         0.75320513,  1.        ],\n",
       "       [ 0.64      ,  0.77777778,  1.        ,  0.875     ,  0.75      ,\n",
       "         0.68589744,  1.        ],\n",
       "       [ 0.36      ,  0.55555556,  0.5       ,  0.625     ,  0.625     ,\n",
       "         0.45512821,  0.        ],\n",
       "       [ 0.9       ,  0.88888889,  1.        ,  1.        ,  1.        ,\n",
       "         0.88461538,  1.        ],\n",
       "       [ 0.64      ,  0.62962963,  0.5       ,  0.625     ,  0.375     ,\n",
       "         0.59935897,  1.        ],\n",
       "       [ 0.66      ,  0.7037037 ,  1.        ,  0.75      ,  0.875     ,\n",
       "         0.63461538,  0.        ],\n",
       "       [ 1.        ,  1.        ,  1.        ,  0.875     ,  0.875     ,\n",
       "         0.8974359 ,  1.        ],\n",
       "       [ 0.62      ,  0.51851852,  0.25      ,  0.25      ,  0.125     ,\n",
       "         0.52564103,  0.        ],\n",
       "       [ 0.56      ,  0.51851852,  0.5       ,  0.5       ,  0.625     ,\n",
       "         0.47115385,  1.        ],\n",
       "       [ 0.44      ,  0.62962963,  0.25      ,  0.625     ,  0.5       ,\n",
       "         0.55448718,  0.        ],\n",
       "       [ 0.7       ,  0.7037037 ,  0.75      ,  0.75      ,  0.75      ,\n",
       "         0.70512821,  1.        ],\n",
       "       [ 0.38      ,  0.25925926,  0.25      ,  0.5       ,  0.5       ,\n",
       "         0.41666667,  0.        ],\n",
       "       [ 0.52      ,  0.37037037,  0.5       ,  0.625     ,  0.25      ,\n",
       "         0.28205128,  0.        ],\n",
       "       [ 0.74      ,  0.74074074,  0.75      ,  0.875     ,  0.875     ,\n",
       "         0.74038462,  1.        ],\n",
       "       [ 0.84      ,  0.92592593,  1.        ,  1.        ,  1.        ,\n",
       "         0.91025641,  1.        ],\n",
       "       [ 0.22      ,  0.48148148,  0.75      ,  0.375     ,  0.5       ,\n",
       "         0.53525641,  0.        ],\n",
       "       [ 0.1       ,  0.2962963 ,  0.25      ,  0.375     ,  0.25      ,\n",
       "         0.33974359,  0.        ],\n",
       "       [ 0.58      ,  0.62962963,  0.5       ,  0.625     ,  0.625     ,\n",
       "         0.71794872,  0.        ],\n",
       "       [ 0.44      ,  0.51851852,  0.5       ,  0.5       ,  0.25      ,\n",
       "         0.3525641 ,  1.        ],\n",
       "       [ 0.38      ,  0.48148148,  0.25      ,  0.375     ,  0.375     ,\n",
       "         0.38461538,  0.        ],\n",
       "       [ 0.46      ,  0.51851852,  0.25      ,  0.375     ,  0.25      ,\n",
       "         0.54487179,  1.        ],\n",
       "       [ 0.42      ,  0.2962963 ,  0.5       ,  0.25      ,  0.375     ,\n",
       "         0.26923077,  1.        ],\n",
       "       [ 0.22      ,  0.40740741,  0.25      ,  0.625     ,  0.625     ,\n",
       "         0.34935897,  1.        ],\n",
       "       [ 0.36      ,  0.37037037,  0.25      ,  0.375     ,  0.75      ,\n",
       "         0.5       ,  1.        ],\n",
       "       [ 0.64      ,  0.62962963,  0.5       ,  0.5       ,  0.625     ,\n",
       "         0.38461538,  0.        ],\n",
       "       [ 0.84      ,  0.85185185,  1.        ,  1.        ,  1.        ,\n",
       "         0.79487179,  1.        ],\n",
       "       [ 0.42      ,  0.44444444,  0.25      ,  0.5       ,  0.25      ,\n",
       "         0.42307692,  1.        ],\n",
       "       [ 0.48      ,  0.51851852,  0.5       ,  0.5       ,  0.625     ,\n",
       "         0.43910256,  1.        ],\n",
       "       [ 0.28      ,  0.25925926,  0.75      ,  0.125     ,  0.375     ,\n",
       "         0.33333333,  0.        ],\n",
       "       [ 0.58      ,  0.44444444,  0.75      ,  0.75      ,  0.875     ,\n",
       "         0.59615385,  1.        ],\n",
       "       [ 0.16      ,  0.2962963 ,  0.75      ,  0.375     ,  0.875     ,\n",
       "         0.28525641,  1.        ],\n",
       "       [ 0.88      ,  0.85185185,  0.75      ,  0.75      ,  0.625     ,\n",
       "         0.87820513,  1.        ],\n",
       "       [ 0.32      ,  0.44444444,  0.25      ,  0.375     ,  0.5       ,\n",
       "         0.45512821,  1.        ],\n",
       "       [ 0.24      ,  0.2962963 ,  0.25      ,  0.375     ,  0.625     ,\n",
       "         0.37179487,  0.        ],\n",
       "       [ 0.74      ,  0.74074074,  0.75      ,  0.625     ,  0.5       ,\n",
       "         0.60576923,  1.        ],\n",
       "       [ 0.38      ,  0.44444444,  0.75      ,  0.625     ,  0.25      ,\n",
       "         0.44230769,  0.        ],\n",
       "       [ 0.54      ,  0.51851852,  0.5       ,  0.75      ,  0.5       ,\n",
       "         0.60897436,  0.        ],\n",
       "       [ 0.16      ,  0.44444444,  0.5       ,  0.625     ,  0.75      ,\n",
       "         0.55769231,  0.        ],\n",
       "       [ 0.5       ,  0.51851852,  0.25      ,  0.75      ,  0.5       ,\n",
       "         0.54487179,  1.        ],\n",
       "       [ 0.5       ,  0.44444444,  0.25      ,  0.5       ,  0.5       ,\n",
       "         0.49358974,  0.        ],\n",
       "       [ 0.42      ,  0.40740741,  0.5       ,  0.625     ,  0.25      ,\n",
       "         0.44871795,  1.        ],\n",
       "       [ 0.62      ,  0.66666667,  0.5       ,  0.625     ,  0.75      ,\n",
       "         0.65064103,  1.        ],\n",
       "       [ 0.46      ,  0.03703704,  0.25      ,  0.375     ,  0.125     ,\n",
       "         0.42628205,  0.        ],\n",
       "       [ 0.18      ,  0.33333333,  0.5       ,  0.75      ,  0.625     ,\n",
       "         0.58333333,  0.        ],\n",
       "       [ 0.34      ,  0.2962963 ,  0.5       ,  0.75      ,  0.5       ,\n",
       "         0.44871795,  0.        ],\n",
       "       [ 0.3       ,  0.33333333,  0.25      ,  0.25      ,  0.375     ,\n",
       "         0.44230769,  0.        ],\n",
       "       [ 0.16      , -0.03703704,  0.        ,  0.25      ,  0.25      ,\n",
       "         0.34615385,  0.        ],\n",
       "       [ 0.66      ,  0.62962963,  0.75      ,  0.75      ,  1.        ,\n",
       "         0.66666667,  1.        ],\n",
       "       [ 0.62      ,  0.33333333,  0.5       ,  0.625     ,  0.75      ,\n",
       "         0.70833333,  1.        ],\n",
       "       [ 0.58      ,  0.44444444,  0.5       ,  0.5       ,  0.625     ,\n",
       "         0.59935897,  1.        ],\n",
       "       [ 0.34      ,  0.59259259,  0.5       ,  0.75      ,  0.5       ,\n",
       "         0.38461538,  1.        ],\n",
       "       [ 0.74      ,  0.66666667,  0.75      ,  0.5       ,  0.75      ,\n",
       "         0.51282051,  1.        ],\n",
       "       [ 0.34      ,  0.62962963,  0.75      ,  0.75      ,  0.875     ,\n",
       "         0.50320513,  0.        ],\n",
       "       [ 0.66      ,  0.74074074,  0.75      ,  0.75      ,  0.875     ,\n",
       "         0.77884615,  1.        ],\n",
       "       [ 0.68      ,  0.51851852,  1.        ,  0.625     ,  0.75      ,\n",
       "         0.59615385,  1.        ],\n",
       "       [ 0.54      ,  0.48148148,  0.        ,  0.125     ,  0.625     ,\n",
       "         0.2724359 ,  1.        ],\n",
       "       [ 0.58      ,  0.33333333,  0.5       ,  0.375     ,  0.375     ,\n",
       "         0.50320513,  0.        ],\n",
       "       [ 0.14      ,  0.2962963 ,  0.5       ,  0.25      ,  0.75      ,\n",
       "         0.27884615,  1.        ],\n",
       "       [ 0.94      ,  0.92592593,  0.75      ,  0.875     ,  0.875     ,\n",
       "         0.91346154,  1.        ],\n",
       "       [ 0.16      ,  0.14814815,  0.25      ,  0.25      ,  0.5       ,\n",
       "         0.13141026,  0.        ],\n",
       "       [ 0.68      ,  0.74074074,  1.        ,  0.75      ,  1.        ,\n",
       "         0.78525641,  1.        ],\n",
       "       [ 0.56      ,  0.59259259,  0.        ,  0.625     ,  0.625     ,\n",
       "         0.74358974,  0.        ],\n",
       "       [ 0.24      ,  0.22222222,  0.5       ,  0.375     ,  0.5       ,\n",
       "         0.20833333,  0.        ],\n",
       "       [ 0.6       ,  0.7037037 ,  0.25      ,  0.625     ,  0.625     ,\n",
       "         0.63461538,  1.        ],\n",
       "       [ 0.3       ,  0.7037037 ,  0.5       ,  0.5       ,  0.625     ,\n",
       "         0.59294872,  0.        ],\n",
       "       [ 0.74      ,  0.81481481,  0.75      ,  0.625     ,  0.75      ,\n",
       "         0.75      ,  0.        ],\n",
       "       [ 0.66      ,  0.62962963,  1.        ,  0.75      ,  1.        ,\n",
       "         0.69871795,  1.        ],\n",
       "       [ 0.16      ,  0.18518519,  0.25      ,  0.125     ,  0.375     ,\n",
       "         0.22435897,  1.        ],\n",
       "       [ 0.84      ,  0.7037037 ,  0.        ,  0.125     ,  0.5       ,\n",
       "         0.59615385,  1.        ],\n",
       "       [ 0.52      ,  0.18518519,  0.        ,  0.125     ,  0.25      ,\n",
       "         0.20192308,  0.        ],\n",
       "       [ 0.44      ,  0.2962963 ,  0.25      ,  0.375     ,  0.625     ,\n",
       "         0.3974359 ,  1.        ],\n",
       "       [ 0.24      ,  0.33333333,  0.5       ,  0.625     ,  1.        ,\n",
       "         0.49038462,  0.        ],\n",
       "       [ 0.64      ,  0.7037037 ,  0.75      ,  0.625     ,  0.375     ,\n",
       "         0.71153846,  1.        ],\n",
       "       [ 0.44      ,  0.40740741,  0.5       ,  0.625     ,  0.625     ,\n",
       "         0.51923077,  0.        ],\n",
       "       [ 0.6       ,  0.62962963,  1.        ,  1.        ,  1.        ,\n",
       "         0.76923077,  1.        ],\n",
       "       [ 0.7       ,  0.62962963,  0.75      ,  0.875     ,  0.75      ,\n",
       "         0.69230769,  1.        ],\n",
       "       [ 0.3       ,  0.48148148,  0.25      ,  0.5       ,  0.5       ,\n",
       "         0.43589744,  0.        ],\n",
       "       [ 0.3       ,  0.44444444,  0.25      ,  0.5       ,  0.25      ,\n",
       "         0.45833333,  0.        ],\n",
       "       [ 0.68      ,  0.7037037 ,  0.75      ,  0.875     ,  0.75      ,\n",
       "         0.77564103,  1.        ],\n",
       "       [ 0.9       ,  0.81481481,  0.75      ,  0.875     ,  0.875     ,\n",
       "         0.92307692,  1.        ],\n",
       "       [ 0.46      ,  0.37037037,  0.5       ,  0.75      ,  0.75      ,\n",
       "         0.625     ,  0.        ],\n",
       "       [ 0.24      ,  0.22222222,  0.        ,  0.25      ,  0.25      ,\n",
       "         0.14423077,  0.        ],\n",
       "       [ 0.64      ,  0.62962963,  1.        ,  0.875     ,  0.75      ,\n",
       "         0.69551282,  0.        ],\n",
       "       [ 0.46      ,  0.33333333,  0.5       ,  0.25      ,  0.5       ,\n",
       "         0.47115385,  0.        ],\n",
       "       [ 0.54      ,  0.62962963,  0.5       ,  0.75      ,  0.875     ,\n",
       "         0.74038462,  1.        ],\n",
       "       [ 0.44      ,  0.40740741,  0.5       ,  0.625     ,  0.75      ,\n",
       "         0.41346154,  0.        ],\n",
       "       [ 0.14      ,  0.11111111,  0.25      ,  0.375     ,  0.25      ,\n",
       "         0.20192308,  0.        ],\n",
       "       [ 0.7       ,  0.77777778,  1.        ,  0.75      ,  1.        ,\n",
       "         0.8525641 ,  1.        ],\n",
       "       [ 0.32      ,  0.48148148,  0.25      ,  0.25      ,  0.375     ,\n",
       "         0.42948718,  0.        ],\n",
       "       [ 0.82      ,  0.81481481,  1.        ,  0.75      ,  0.625     ,\n",
       "         0.84615385,  1.        ],\n",
       "       [ 0.3       ,  0.55555556,  1.        ,  0.5       ,  0.5       ,\n",
       "         0.53846154,  0.        ],\n",
       "       [ 0.38      ,  0.40740741,  0.25      ,  0.25      ,  0.375     ,\n",
       "         0.46794872,  0.        ],\n",
       "       [ 0.4       ,  0.44444444,  0.25      ,  0.5       ,  0.625     ,\n",
       "         0.38782051,  0.        ],\n",
       "       [ 0.24      ,  0.62962963,  0.5       ,  0.75      ,  0.875     ,\n",
       "         0.54487179,  0.        ],\n",
       "       [ 0.88      ,  0.77777778,  0.75      ,  0.75      ,  0.75      ,\n",
       "         0.84294872,  1.        ],\n",
       "       [ 0.48      ,  0.33333333,  0.75      ,  0.375     ,  0.25      ,\n",
       "         0.34615385,  1.        ],\n",
       "       [ 0.6       ,  0.62962963,  1.        ,  1.        ,  0.875     ,\n",
       "         0.77564103,  1.        ],\n",
       "       [ 0.46      ,  0.48148148,  0.25      ,  0.375     ,  0.25      ,\n",
       "         0.5224359 ,  0.        ],\n",
       "       [ 0.62      ,  0.66666667,  1.        ,  1.        ,  1.        ,\n",
       "         0.84935897,  1.        ],\n",
       "       [ 0.08      ,  0.07407407,  0.        ,  0.125     ,  0.125     ,\n",
       "         0.26923077,  0.        ],\n",
       "       [ 0.4       ,  0.22222222,  0.25      ,  0.125     ,  0.25      ,\n",
       "         0.16025641,  0.        ],\n",
       "       [ 0.52      ,  0.44444444,  0.25      ,  0.375     ,  0.375     ,\n",
       "         0.44871795,  1.        ],\n",
       "       [ 0.54      ,  0.51851852,  0.25      ,  0.625     ,  0.5       ,\n",
       "         0.47435897,  0.        ],\n",
       "       [ 0.5       ,  0.48148148,  0.5       ,  0.875     ,  0.625     ,\n",
       "         0.51923077,  0.        ],\n",
       "       [ 0.12      ,  0.22222222,  0.25      ,  0.5       ,  0.625     ,\n",
       "         0.15384615,  0.        ],\n",
       "       [ 0.82      ,  0.85185185,  1.        ,  1.        ,  1.        ,\n",
       "         0.82692308,  1.        ],\n",
       "       [ 0.4       ,  0.2962963 ,  0.5       ,  0.625     ,  1.        ,\n",
       "         0.59294872,  1.        ],\n",
       "       [ 0.64      ,  0.62962963,  1.        ,  1.        ,  0.75      ,\n",
       "         0.73717949,  1.        ],\n",
       "       [ 0.44      ,  0.51851852,  0.25      ,  0.375     ,  0.625     ,\n",
       "         0.47115385,  0.        ],\n",
       "       [ 0.2       ,  0.33333333,  0.25      ,  0.125     ,  0.25      ,\n",
       "         0.34294872,  0.        ],\n",
       "       [ 0.52      ,  0.25925926,  0.25      ,  0.125     ,  0.5       ,\n",
       "         0.43589744,  1.        ],\n",
       "       [ 0.52      ,  0.33333333,  0.5       ,  0.25      ,  0.5       ,\n",
       "         0.19230769,  0.        ],\n",
       "       [ 0.42      ,  0.22222222,  0.25      ,  0.375     ,  0.5       ,\n",
       "         0.37820513,  0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, will again import the necessary library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use activation = \"relu\" for \"hidden Layer\", as it is the best activation function.  \n",
    "And activation= \"linear\" for output, as we can see that it is \"regression model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, activation = 'relu', input_dim = 7))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(1,activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                80        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 201 (804.00 Byte)\n",
      "Trainable params: 201 (804.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use loss = \"mean_squared_error\", because it is a \"Regression model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= 'mean_squared_error', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 1s 27ms/step - loss: 0.1354 - val_loss: 0.0757\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0529 - val_loss: 0.0262\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0224 - val_loss: 0.0184\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0211 - val_loss: 0.0199\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0206 - val_loss: 0.0173\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0184 - val_loss: 0.0162\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0178 - val_loss: 0.0157\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0173 - val_loss: 0.0151\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0168 - val_loss: 0.0147\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0162 - val_loss: 0.0143\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0157 - val_loss: 0.0139\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0153 - val_loss: 0.0134\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0148 - val_loss: 0.0130\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0143 - val_loss: 0.0126\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0139 - val_loss: 0.0122\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0135 - val_loss: 0.0119\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0131 - val_loss: 0.0115\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0127 - val_loss: 0.0112\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0124 - val_loss: 0.0108\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0120 - val_loss: 0.0103\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0115 - val_loss: 0.0100\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0102 - val_loss: 0.0092\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0097 - val_loss: 0.0088\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0088 - val_loss: 0.0079\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0075\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0071\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0068\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0064\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0062\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0066 - val_loss: 0.0060\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0063 - val_loss: 0.0055\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0052\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0049\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0044 - val_loss: 0.0046\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0044 - val_loss: 0.0046\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0045\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0045\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0045\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0046\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0042 - val_loss: 0.0044\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0042 - val_loss: 0.0044\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0044\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0045\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0044\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0042 - val_loss: 0.0044\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0043\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0044\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0044\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0042\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scale, y_train, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6266911 ],\n",
       "       [0.71581036],\n",
       "       [0.5465287 ],\n",
       "       [0.66530377],\n",
       "       [0.8467781 ],\n",
       "       [0.78253156],\n",
       "       [0.6970584 ],\n",
       "       [0.74889034],\n",
       "       [0.5421228 ],\n",
       "       [0.59802866],\n",
       "       [0.90144837],\n",
       "       [0.5623512 ],\n",
       "       [0.80834544],\n",
       "       [0.81835   ],\n",
       "       [0.63486445],\n",
       "       [0.95588225],\n",
       "       [0.73323435],\n",
       "       [0.78860146],\n",
       "       [0.97249085],\n",
       "       [0.6250183 ],\n",
       "       [0.722482  ],\n",
       "       [0.6776745 ],\n",
       "       [0.8179384 ],\n",
       "       [0.60836536],\n",
       "       [0.53627336],\n",
       "       [0.8439045 ],\n",
       "       [0.96748596],\n",
       "       [0.68350047],\n",
       "       [0.5445355 ],\n",
       "       [0.7727718 ],\n",
       "       [0.62629247],\n",
       "       [0.58869725],\n",
       "       [0.6937131 ],\n",
       "       [0.5964919 ],\n",
       "       [0.62759334],\n",
       "       [0.66868836],\n",
       "       [0.6138492 ],\n",
       "       [0.8997701 ],\n",
       "       [0.62134   ],\n",
       "       [0.7039754 ],\n",
       "       [0.5585183 ],\n",
       "       [0.7412113 ],\n",
       "       [0.5596144 ],\n",
       "       [0.92454755],\n",
       "       [0.67187583],\n",
       "       [0.59551555],\n",
       "       [0.77657026],\n",
       "       [0.6283037 ],\n",
       "       [0.714076  ],\n",
       "       [0.7060472 ],\n",
       "       [0.6690328 ],\n",
       "       [0.6446927 ],\n",
       "       [0.6307438 ],\n",
       "       [0.78552943],\n",
       "       [0.564635  ],\n",
       "       [0.72632366],\n",
       "       [0.65507495],\n",
       "       [0.6118485 ],\n",
       "       [0.526952  ],\n",
       "       [0.8059696 ],\n",
       "       [0.7509307 ],\n",
       "       [0.7371234 ],\n",
       "       [0.61448234],\n",
       "       [0.7766666 ],\n",
       "       [0.6976713 ],\n",
       "       [0.8651956 ],\n",
       "       [0.76133543],\n",
       "       [0.66348743],\n",
       "       [0.6513606 ],\n",
       "       [0.55763626],\n",
       "       [0.9588536 ],\n",
       "       [0.4760776 ],\n",
       "       [0.8606915 ],\n",
       "       [0.74705017],\n",
       "       [0.5293707 ],\n",
       "       [0.7440824 ],\n",
       "       [0.6976797 ],\n",
       "       [0.8174109 ],\n",
       "       [0.8064526 ],\n",
       "       [0.5264968 ],\n",
       "       [0.8077054 ],\n",
       "       [0.46707335],\n",
       "       [0.6439717 ],\n",
       "       [0.7056607 ],\n",
       "       [0.8108128 ],\n",
       "       [0.68966776],\n",
       "       [0.8660814 ],\n",
       "       [0.80553645],\n",
       "       [0.6120353 ],\n",
       "       [0.6128126 ],\n",
       "       [0.8534766 ],\n",
       "       [0.96046084],\n",
       "       [0.7479258 ],\n",
       "       [0.44800702],\n",
       "       [0.8202142 ],\n",
       "       [0.6494508 ],\n",
       "       [0.8199646 ],\n",
       "       [0.64957577],\n",
       "       [0.49705595],\n",
       "       [0.8915735 ],\n",
       "       [0.6106102 ],\n",
       "       [0.9158779 ],\n",
       "       [0.7030679 ],\n",
       "       [0.6236655 ],\n",
       "       [0.59779096],\n",
       "       [0.692583  ],\n",
       "       [0.90978247],\n",
       "       [0.6388634 ],\n",
       "       [0.86421824],\n",
       "       [0.6380427 ],\n",
       "       [0.9151929 ],\n",
       "       [0.49031332],\n",
       "       [0.47604004],\n",
       "       [0.68385077],\n",
       "       [0.6385694 ],\n",
       "       [0.68302387],\n",
       "       [0.49227604],\n",
       "       [0.91706765],\n",
       "       [0.68344873],\n",
       "       [0.83784336],\n",
       "       [0.6481305 ],\n",
       "       [0.5607619 ],\n",
       "       [0.6379989 ],\n",
       "       [0.51753557],\n",
       "       [0.5897861 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test_scale)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use \"r2 score\" to goodness of our prediction.  \n",
    "We can't use \"accuracy\", as it is not be used for \"Regression model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.741874469628151"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e8655b8190>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
